---
title: "Personal Learning: The Effects of Class Size and Urbanicity on 1st Grade Math Performance in STAR Study"
author: "Benjamin Jewell"
date: "March 18th 2024"
output:
  html_document:
    df_print: paged
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r, echo=F, message=F, error=F, warning=F}
library(AER)
library(ggplot2)
library(tidyverse)
library(foreign)
library(gplots)
library(car)
library(FSA)
library(knitr)

STAR = read.spss("../STAR_Students.sav", to.data.frame = T)
```

------------------------------------------------------------------------

------------------------------------------------------------------------

# Abstract

The original STAR study worked to find if the size of a classroom for
kindergartners through first graders had an impact on their future
academic performance. In this paper we will study if the classroom size
and setting of the school impacts their math test score. The median math
score of students grouped by their teacher was used to measure their
performance. A fixed effects model was fit to the data, though
deviations from the normality assumption were detected so non-parametric
tests were employed. Both classroom size and school setting were found
to be impactful to first grade math scores, and among class types it was
concluded that small classrooms lead to higher math scores compared to
other class types.

# Introduction

This study is seeking to answer two questions: Is any differences in
math scaled scores in 1st grade across class types? As well as, which
class type is associated with the highest math scaled scores in 1st
grade?One can likely believe anecdotally that in a class packed with
students and a sole teacher the quantity of education is likely
diminished compared a class where students get individual attention.
This study seeks to examine this belief, in comparing both small classes
to regular classes, and regular classes with regular classes that also
have a teacher's aide. Both of these approaches lead to balancing the
student to educator ratio, though in different manners. In performing
this study we can hope to discover a better way to educate students to
ensure their academic success. Not only can this help their future
careers and opportunities, but also leads to a more educated future for
society.

# Background

Known as the Tennesses Student/Teacher Achievement Ratio study (STAR),
this project focused on the benefits of classroom sizes for students and
was a study conducted in 1985 to 1989. The study focused on students in
kindergarten and first through third grade, recording their math and
reading scores those four years. Data was collected for a wide number of
factors, from class size, to the education of a given teacher, and many
more. Further analysis would be conducted over twenty five years to
examine the long term benefits for these students.

The STAR study sought to further understand the difference in classroom
size, which until then had confusing and unclear data due to various
counting methods (Achilles, 2012, p. 1). In performing their own
experiment the study sought to find a clear and definitive answer to the
effects of classroom size on the education of their students. In
conducting this experiment it was hoped that a stronger method for
teaching children could be discovered to better not only their own
educational and professional achievements, but also to help fashion a
more educated society.

# STAR Experimental Design

The target students in this study were in seventy nine different schools
throughout Tennessee in kindergarten, who would be tracked for their
adolescent academic lives. These schools used in the study were of a
variety of types including inner city schools, rural schools, urban
school and suburban schools. Any appropriate school within Tennessee
could join the study, allowing for a broader scope in what schools were
included. Schools had to meet a minimum pupil count in the first year to
join, allowing for seventy nine schools to be part of the study the
first year. Due to the variety of schools and locations used the study
believes it has allowed for a reasonable representation of different
incomes and ethnicity among the students (Achilles, 2012, p. 6).

The study tracked students starting in kindergarten, with varying
classroom types prescribed to them by the STAR project through third
grade. With 7,000 students to begin with, these students were each
randomly assigned to one of three classroom types: small classrooms
(15-17 students), regular classrooms (22-25 students) and regular
classrooms with an aide to assist the teacher. Similarly to the
students, teachers were randomly assigned classroom types to avoid any
issues with teachers specializing in teaching particular sizes of
classrooms. Besides the change to classroom size no other changes were
made to allow for a normal school environment.

Compared to many similar studies the STAR project sought to focus
specifically on the size of a given class rather than the pupil-teacher
ration (PTR). This allows for a clear understanding of how many students
are involved in a given classroom, for a class with two teachers and
sixty students will have the same PTR as a class with one student and
thirty students. The use of classroom size in STAR allows for a more
direct analysis on the educational benefits of the number of students in
a given classroom.

It should be noted that while the STAR program began with 7,000 students
they did not retain all students over the course of the study. As the
program did not force students to stay in the project students might
leave throughout the study for a variety of factors such as moving to a
different city or state, or having to drop out of school. By beginning
first grade three of the seventy nine schools had dropped from the
program.

Not only did students leave the program, but because the STAR study
examined specific schools and not students, any students who joined a
STAR school were enrolled into the study. These students would not have
had the controlled classroom size from previous years of the study, and
could only benefits from the years they were in the study but no note is
made to exclude them. To compound this issue, at the time this study was
conducted kindergarten was not mandatory in Tennessee, meaning many
students joined the program for the first time in first grade and missed
a whole year of STAR class size.

[Why do we still use the study?]

# Initial Analysis

Previous examinations have been made of this data set in an initial
analysis report. Initial inquiries into the first grade math scores
looked for a relationship between the class type and specific schools.
This investigation used an additive two way ANOVA model with no
interaction. While the model indicated that both class type and school
ID had an effect on the first grade math scores there were several
issues with the model used. In investigating model assumptions several
were found to be broken, namely the normality and equal variance. As
such any conclusions that this initial analysis report might have made
are not valid, only serving to familiarize us with the problems we will
face in this study.

# Descriptive analysis

For our analysis we will be concerning ourselves with five variables, ,
`g1tmathss` , `g1tchid`, `g1classtype`, `g1schid`, and `g1surban`. Each
of these five variables corresponds to a student in the first grade,
tracking their progress in the STAR project. In order our five variables
are:

-   `g1tmathss`: The math score of a given student in the first grade.

-   `g1schid`: The ID of the school a given student is at.

-   `g1tchid`: The ID of the teacher who is teaching a given student.

-   `g1classtype`: The type of class the student is enrolled in, with
    levels `small`, `regular` and `regular+aide`.

-   `g1urban`: The urbanicity of the school the student is enrolled in,
    with levels `inner city`, `suburban`, `rural` and `urban`.

## Summary of 1st Grade Math Scores

We can see that we have a mean math score among first grade students of
530.53, a median score of 529, a lower quartile of 500 and higher
quartile of 557. There is a standard deviation of 43.11 among the grade
1 math scores. Of particular note is the large amount of missing data
points in the data, as denoted by `Total_NAs` below, making up more than
a quarter of the data. This is no insignificant portion of the data, so
we will need to investigate this further below.

```{r, warning=F, error=F, message=F, echo = F}
#Summary stastistics of 1st year math scores
g1smath_stats = data.frame(
        Mean = mean(STAR$g1tmathss, na.rm = T),  
        Stddev = sd(STAR$g1tmathss, na.rm = T),
        q25 = quantile(STAR$g1tmathss, probs = 0.25, na.rm = T), 
        q50 = quantile(STAR$g1tmathss, probs = 0.50, na.rm = T), 
        q75 = quantile(STAR$g1tmathss, probs = 0.75, na.rm = T))
colnames(g1smath_stats) = c('Mean', 'Standard Deviation', 'Q25%', 'Q50%', 'Q75%')
rownames(g1smath_stats) = c('')
kable(g1smath_stats, captain = 'Table 1: Summary statistics of 1st grade math scores')

  
kable(c(Total_NAs = sum(is.na(STAR$g1tmathss)), #How many NAs in math scores
Total_data = length(STAR$g1tmathss)),
col.names = 'Missing Values',
caption = 'Table 2: Missing Values Summary')

```

## Data Abnormalities

### STAR Class Representation

As we are looking at the effects of each class type on all given schools
we want to guarantee that all schools have all three class types
present. If one or more class types are missing we cannot have the full
picture of the effects on math scores at that particular school, we will
be missing the full picture. As such schools that do not have all three
class types present should be dropped from our investigation. It was
found that four such schools exist, specifically schools with ID 244728,
244736, 244796 and 244839

```{r, warning=F, error=F, message=F, echo = F}
classlist = STAR %>%
  group_by(g1schid, g1classtype) %>%
  summarise(classnum = 1)

#TODO: Hide this table
t = table(classlist$g1schid)
data.frame(t[which(t != 3)])

#We will drop these 4 schools later when we assemble the STARt data frame
```

### Analysis of Missing Values

Looking at the math scores it can be noted that 43.12% (3043 students)
of these values are missing. However upon investigating, a majority of
these missing values are from students who have left the STAR program by
the first grade. Removing these non-STAR students from our data we can
find that this reduces the number of missing values to 3.38% (256
students), a much smaller proportion of the data.

```{r, echo = F}
#How many math scores are NA out of the total?
tot_raw_na = sum(is.na(STAR$g1tmathss)) / length(STAR$g1tmathss) #BAD!

#Alternatively using the STARGS1 Flag 
tot_star_na = sum(is.na(STAR$g1tmathss[which(STAR$FLAGSG1 == 'YES')])) / length(STAR$g1tmathss[which(STAR$FLAGSG1 == 'YES')])


#Number of non-NA values in g1math
not_na = 1 - tot_raw_na
#Number of NA values in g1math that are enrolled in star (FLAGSG1 = T)
star_na = sum(is.na(STAR$g1tmathss[which(STAR$FLAGSG1 == 'YES')])) / length(STAR$g1tmathss)
#Number of NA values in g1math NOT enrolled in star
not_star_na = tot_raw_na - star_na 


#Piechart of missing proportions
tot_star_df = data.frame(group = c('Missing Value in STAR', 'Existing Values in STAR', 'Missing Values not in STAR'),
  perc = c(star_na, not_na, not_star_na))


#Pie chats for data
ggplot(tot_star_df, aes(x="", y=perc, fill = group)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start=0) +
  xlab('') + ylab('') +
  ggtitle('Figure XX: Proportion of Missing 1st Grade Math Scores')

#Table of values
na_df = data.frame(c(tot_raw_na, tot_star_na))
colnames(na_df) = c('Percent')
rownames(na_df) = c('Percentage of Missing Math Values', 'Percentage of missing Math values of enrolled stuents')
kable(na_df, caption = 'Table 3: Missing Values for STAR and non-STAR students')
```

```{r}
#MISSING VALUES EXAMINATION

#What are the missing values?
miss3 = STAR[which(STAR$FLAGSG1 == 'YES'), ]
miss3 = miss3[is.na(miss3$g1tmathss), ]
#miss3

#lapply(miss3, table)
table(miss3$gksurban)
table(miss3$g1surban)
```

### Dropping Data Points

Dropping this small segment of the data likely won't have any major
effect, however to be sure we can looking into the number of missing
values per school. As seen in Figure 1 most school have five or less
missing values, with one outlying school missing twenty eight values.
For this outlying school however these missing values only make up
22.40% of the students here. By comparison this is a large portion of
missing students, however even with 28 missing students there are still
97 students enrolled in the program. Considering that the STAR program
required a minimum of 57 students to be part of the project this reduced
number is still well above the threshold.

Due to these findings we will chose to drop any student who has a
missing value for their 1st grade math score. A majority of the students
dropped from our study will be those who are no longer enrolled in the
STAR program by first grade, reducing our missing values from making up
43.12% of our data to just 3.38% of the data (Table 3).

As seen in Figure AA few schools are missing more than 5 values, and we
have addressed the number of students remaining at school with 28
missing students (School 168211). Even if all remaining schools are
missing the maximum number of values 9 students, as seen in Figure AA,
and began with the minimum number of students per STAR protocol (57
students), then 48 students would remain at that location. This extreme
example highlights that in dropping these remaining students with
missing math scores would not impact our study significantly.

With these considerations in mind we will choose to only examine
students who have math scores in the first grade, and drop any students
who are missing first grade math scores.

```{r, warning=F, error=F, message=F, echo = F}
#Do any schools have more NA values than others?
na_star = STAR[which(STAR$FLAGSG1 == 'YES'),] %>%
  group_by(g1schid, g1tmathss) %>%
  summarize(na_count = sum(is.na(g1tmathss)))

hist(na_star$na_count[which(is.na(na_star$g1tmathss))], breaks = 1:30,
     xlab = 'Frequecy of NA values',
     main = 'Figure AA: Frequency of missing values among Schools')

table(na_star$na_count[which(is.na(na_star$g1tmathss))])

#Notice one school has 28 missing, out of 125
miss28_proportion = 28 / length(STAR$g1schid[which(STAR$g1schid == 168211)])
schid168211 = STAR[which(STAR$g1schid == 168211), ]

#Seven students removed from eacher teacher???
#table(schid168211$g1tchid)
#table(schid168211$g1tchid[is.na(schid168211$g1tmathss)])

```

## Aggregated Teacher Statistic

In examining the 1st grade math scores between students we cannot simply
use the individual scores due to high correlation between students in a
single class. Because they are taught in the same setting and in the
same manner a high correlation develops between them, instead the median
score of such a class could be considered to deal with this problem. To
take the problem a step further, students taught by the same teacher are
likely to have issues of correlation as well since teaching style likely
only differs so much between classes taught by the same educator.

Taking this all into consideration we will be using the median score of
all students taught by a single teacher as our statistic of interest in
this study. The median was chosen over the mean as it will not be
effected by outliers, and in previous investigations it was found that
using the mean results in a model that deviates from key model
assumptions.

```{r, warning=F, error=F, message=F, echo = F}
STARt = STAR %>% 
  group_by(g1tchid, g1classtype, g1schid, g1surban) %>%
  drop_na(g1tmathss) %>%
  summarize(medn_score = median(g1tmathss, na.rm = T))

#Drop our 4 schools missing all data types
STARt = STARt[-which(STARt$g1schid %in% c(244728, 244736, 244796, 244839)),]

STARt$g1schid = as.factor(STARt$g1schid)
STARt$g1tchid = as.factor(STARt$g1tchid)

```

### Median Math Score: Summary Statistics

Examining the new median statistic score per teacher we can find that
there is a median score of 532, a standard deviation of 26.74, a lower
quarter of 512 and upper quartile of 549. As seen in Figure 2 the median
math score per teacher is somewhat normally distributed, but appears to
have a heavy tail.

```{r, warning=F, error=F, message=F, echo = F}
#Statistics for each Teacher
teacher_dat = data.frame(c(mean(STARt$medn_score),
  sd(STARt$medn_score, na.rm = T),
  q25 = quantile(STARt$medn_score, probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score, probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score, probs = 0.75, na.rm = T))) #75th quantile
colnames(teacher_dat) = c('Median Test Scores per Teacher')
rownames(teacher_dat) = c('Mean', 'Std Dev', 'Q25%', 'Q50%', 'Q75%')
kable(teacher_dat, caption = 'Table 4: Summary Statistics for Median Scores per Teacher')

#Histogram of median math scores per teacher
ggplot(data = STARt, aes(medn_score)) +
  geom_histogram(bins = 30) +
  xlab('Mean Math Scores per Teacher') +
  ggtitle('Figure 2: Histogram of Median Math Scores per Teacher')
```

### Class Types: Summary Statistics

Examining the statistics of various class types we can see in the
boxplot comparison small classes have higher medians and quartiles
compared to the other two class types (Figure 3). Actual testing will
need to be performed to confirm this visual inspection. The various
summary statistics for each class type can be seen in Table 5, which
supports our visual inspection of the boxplots.

```{r, warning=F, error=F, message=F, echo = F}
#Statistics for `small` classes
small_class_stats = c(mean(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS']),
  sd(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS'], na.rm = T), 
  q25 = quantile(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS'], probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS'], probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS'], probs = 0.75, na.rm = T)) #75th quantile 

#Statistics for `regular` classes
reg_class_stats = c(mean(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS']),
  sd(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS'], na.rm = T), 
  q25 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS'], probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS'], probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS'], probs = 0.75, na.rm = T)) #75th quantile 

#Statistics for `regular+aide` classes
rgaide_class_stats = c(mean(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS']), 
  sd(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS'], na.rm = T), 
  q25 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS'], probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS'], probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS'], probs = 0.75, na.rm = T)) #75th quantile 

classdat_df = data.frame(small_class_stats, reg_class_stats, rgaide_class_stats)
colnames(classdat_df) = c('Small Class', 'Regular Class', 'Regular + Aide Class')
rownames(classdat_df) = c('Mean', 'Std Dev', 'Q25%', 'Q50%', 'Q75%')

kable(classdat_df, caption = 'Table 5: Summary Statistics per Class Size')

ggplot(data = STARt, aes(g1classtype, medn_score)) + 
  geom_boxplot() +
  xlab('Class Type') +
  ylab('Average Test score Year 1') +
  ggtitle('Figure 3: Median combined test Score per class type')
```

### School Urbanicity: Summary Statistics

Below we can see the summary statistics for the urbanicity of each
school can be seen in Table 6. As seen in Figure 4 we have compared the
median math scores per individual school, with coloring per school
urbanicity to differentiate between them. Figure 5 displays a boxplot of
median math scores per urbanicity level, where we can see that inner
city schools have lower medians than the other three settings which are
approximately equal.

```{r, warning=F, error=F, message=F, echo = F}
sum_stats = function(lbl){
  c(mean(STARt$medn_score[STARt$g1surban %in% lbl]), 
  sd(STARt$medn_score[STARt$g1surban %in% lbl], na.rm = T),
  q25 = quantile(STARt$medn_score[STARt$g1surban %in% lbl], probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score[STARt$g1surban %in% lbl], probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score[STARt$g1surban %in% lbl], probs = 0.75, na.rm = T)) #75th quantile 
}

#Summary Stats for School Settings
inner_city_dat = sum_stats('INNER CITY') 
suburban_dat = sum_stats('SUBURBAN')
rural_dat = sum_stats('RURAL')
urban_dat = sum_stats('URBAN')

school_setting_df = data.frame(inner_city_dat,
                               suburban_dat,
                               rural_dat,
                               urban_dat
                               )
colnames(school_setting_df) = c('Inner City', 'Suburban', 'Rural', 'Urban')
rownames(school_setting_df) = c('Mean', 'Std Dev', 'Q25%', 'Q50%', 'Q75%')
kable(school_setting_df, caption = 'Table 6: Summary Statistics per School Setting')

sortedSTAR = STARt[order(STARt$g1surban), ]
lvlinfo = sortedSTAR %>% 
      arrange(g1surban, g1schid) %>%
      pull(g1schid) %>%
      unique()

STARt %>%
  mutate(g1surban = as.factor(g1surban),
         g1schid = factor(g1schid, levels = lvlinfo)) %>%
  ggplot(aes(x = g1schid, y = medn_score, fill = g1surban, color = g1surban)) +
  geom_boxplot() +
  xlab('') +
  ylab('') +
  ggtitle('Figure XX: Median Score between Schools') +
  theme(axis.text.x = element_blank())
  
#Boxplot of median score per school
#ggplot(data = sortedSTAR, aes(x = reorder(g1schid, g1surban),, y = medn_score, #color = g1surban)) + 
#  geom_boxplot() + 
#  xlab('1st grade School') +
#  ylab('Average Test score Year 1') +
#  ggtitle('Figure 4: Median Combined Test Score per School Year 1') 

#Boxplot of median score per school setting
#ggplot(data = STARt, aes(g1surban, medn_score, color = g1surban)) + 
#  geom_boxplot() + 
#  xlab('1st grade School') +
#  ylab('Average Test score Year 1') +
#  ggtitle('Figure 5: Median combined 1st grade test score by urbanicity')
```

# Inferential analysis

## Modeling

We will be fitting a fixed effect model for the data given, using the
class type, school ID and school urbanicity as our independent variables
and the median math score per teacher as our response variable. In using
the school ID, the effects of specific schools can be accounted for,
while the urbanicity of the school helps see general effects of school
setting beyond the very specific school ID.

### Model Assumptions

An imbalanced fixed effect model was chosen to be fit to our data, with
the following equations:

$$
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk} \\
i \in \{1,2,3\}, j =1,...,4, k=1,2,...,n
$$

In this case our variables are:

-   $\mu_{..}$ : Mean response across all indexes.

-   $\alpha_i$ : The main effects of the fixed variable school location
    setting.

-   $\beta_j$ : The main effects of the fixed variable class type.

-   $\epsilon_{ijk}$ : The random error for entry (i,j,k).

### Model Fitting

We will be performing an F-test for both of our variables, class type
and school setting, to see if they are significant at $\alpha = 0.05$.
We will be testing the following hypothesis:

$$
H_0: \alpha_i = 0 \text{ }\forall i. \\
H_1: \text{Not all }\alpha_i\text{ equal zero.} 
$$

As well as:

$$
H_0: \beta_j = 0 \text{ }\forall j. \\
H_1: \text{Not all }\beta_j\text{ equal zero.}
$$

As we can our tests return a p-value of XXX, XXX, XXX respectively,
indicating that both factors of school setting and classroom have main
effects. Note that this test assumes normality from the model, which we
will discuss further in the sensitivity section of this report.

A test for interaction terms was also performed at significant level
$\alpha = 0.05$, testing the following hypothesis:

$$
H_0: (\alpha\beta)_{ij} = 0 \\
H_1: \text{not all } (\alpha\beta)_{ij} \text{ are zero.}
$$

Via the F-test we report a p-value of XXX, indicating that we fail to
reject the null hypothesis. As such there is no interaction between
classroom type and school setting and we can safely ignore the need to
add an interaction term to the model.

```{r, warning=F, error=F, message=F, echo = F}
#Anova(lm(medn_score ~ g1classtype + as.factor(g1schid), data = STARt), type = 2)

STAR_medn_aov = aov(medn_score ~ g1classtype + as.factor(g1surban) + as.factor(g1schid), data = STARt)
summary(STAR_medn_aov)

#STAR_medn_aov = lme4::lmer(medn_score ~ g1classtype + (1|as.factor(STARt$g1schid)), data = STARt)

#Should we drop interaction term?
full_model = aov(medn_score ~ g1classtype * as.factor(g1schid) * as.factor(g1surban), data = STARt)
red_model = aov(medn_score ~ g1classtype + as.factor(g1schid) + as.factor(g1surban), data = STARt) 
anova(red_model, full_model)
```

## Effects Plots & Interaction Plot

An investigation into the effects leads to examining the main effects
plot. The main effects of class type can be seen in Figure 6, where it
appears that a smaller class type has a strong effect on median math
score compared to the other two class types, further supporting our
suspicions drawn from the boxplots earlier (Figure 3). This will be of
particular note when answering our secondary question below. In Figure 7
we can see the main effects of the school setting, with a Inner City
schools having a significant decrease in median math scores compared to
the other school locations. Finally an interaction plot is constructed
(Figure 8), allowing us to see that there is little interaction between
school setting and class type, as our F-test previously confirmed.

[WHY NO SCHOOL ID PLOT??]

```{r, warning=F, error=F, message=F, echo = F}
#Effects & Interaction Plots
plotmeans(STARt$medn_score ~ STARt$g1classtype, main = 'Figure 6: Main Effects of Class Type')
plotmeans(STARt$medn_score ~ STARt$g1surban, main = 'Figure 7: Main Effects of School Setting')
interaction.plot(STARt$g1surban, STARt$g1classtype, STARt$medn_score, main = 'Figure 8: Interaction Plot between Class Type and School Setting')
```

## Secondary Question of Interest:

Let us perform the Tukey Range test at significance level
$\alpha = 0.05$. The Tukey Range test examines all pairwise differences
in the means between different levels in a given treatment. This test
has the following assumptions

-   $Y_{ijk}$ are independent within their group and between groups

-   Each group is normally distributed

-   Homogeneity of variance between the group variances.

The Tukey Range test will be used to perform an examination to discover
if any class type has a higher mean summary score than other class
types. In particular we will be testing the following null hypothesis,
$H_0: \mu_i = \mu_{i'}$ where i and i’ are the three levels of the STAR
class type.

```{r}
TukeyHSD(aov(medn_score ~ g1classtype, data = STARt))

hist(STARt$medn_score[which(STARt$g1classtype == 'SMALL CLASS')])
hist(STARt$medn_score[which(STARt$g1classtype == 'REGULAR CLASS')])
hist(STARt$medn_score[which(STARt$g1classtype == 'REGULAR + AIDE CLASS')])
```

~~From the results of this test we can see that there are significant
differences between small classes and regular classes, as well as
regular classes with an aide vs small classes in the mean math score of
those classes. However there does not appear to be such a significant
difference between regular classes and those regular classes with
aides.~~

# Sensitivity analysis

Let us test the various assumptions of our model to ensure that we do
not deviate away from them.

## Normality in Residuals

One assumption of particular note here is that of normality. In
examining our residuals vs fitted plot (figure 9) we can see that there
is little pattern to the data points, indicating no major issues with
variance or mean. However in examining our residuals via histogram and
qqplot (Figure 10, Figure 11) we can see that there is a deviation from
normality. Our histogram is right skewed, one piece of evidence of
departure from normality. This is further confirmed by performing a
Shapiro-Wilks test. At significant level 0.05 the test rejects our null
hypothesis, concluding that our data is non-normal.

While the breaking of the normality of assumption is something to take
note of, it is not as a big of a concern as previously led to believe.
As stated in Glass et al. (1972) "Skewed populations have very little
effect on either the level of significance or the power of the
fixed-effects model F-test" (Glass et al, 273). Because this skewness is
not particularly strong we can move forward knowing this departure of
normality will not strong effect our F-tests.

This effect can be seen between the agreement in our effects and
interaction plots (Figures 6, 7, 8), as well as the Kruskal-Wallis test
with the F-tests. Not only does Glass et al. support us moving forward
with this investigation, the use of non-parametric tests are also
possible and employed in our study.

```{r, warning=F, error=F, message=F, echo = F}
plot(STAR_medn_aov, which = 1, main = 'Figure 9: Median Model Residuals vs Fitted plot')
hist(residuals(STAR_medn_aov), main = 'Figure 10: Histogram of Median Model Residuals', xlab = 'Residuals')
qqnorm(residuals(STAR_medn_aov), main = 'Figure 11: QQplot of Median Model')
qqline(residuals(STAR_medn_aov))

shapiro.test(residuals(STAR_medn_aov)) #P > 0.05, so we can assume normality
```

## Levene Test for Equal Variance

Testing for equal variance among groups, a Levene Test was performed at
significance level 0.05. We will be testing if the variance among
populations is equal, with the follow null and alternative hypothesis.

$$
H_0: \sigma^2_1 = ... = \sigma^2_n
$$

$$
H_1: \sigma^2_i \neq \sigma^2_j \text{ for all } i\neq j
$$

The results of the Levene test indicate no presence of unequal variance
among levels of class type, nor among levels of school setting. As equal
variance is a key assumption in our fixed effects model this test
indicates we need not worry about such depatures.

```{r, warning=F, error=F, message=F, echo = F}
leveneTest(medn_score ~ g1classtype * as.factor(g1surban) * as.factor(g1schid), STARt)
```

## Independence

Unfortunately because we are only involved in analyzing the data and not
designing the experiment we must trust in how the Tennessee general
assembly set up the experiment. Independence can be seen in the random
assignment of class type to each class. Any correlation that might be
present among students of the same teacher has also been eliminated by
the use of our median summary statistic.

## Cook's Distance: Influential Points & Outliers

Using Cook's distance one can look for outliers and influential points
that might effect our model. The following plot, Figure 12, indicates
that there are few points that differ from the major of values. Those
few values that have a large Cook's distance compared to the rest of the
data set have values so small they don't approach being influential,
thus they are neither outliers. Thus no adjustments need to be made to
deal with influential points or outliers.

```{r, warning=F, error=F, message=F, echo = F}
par(mfrow = c(1,2))
plot(STAR_medn_aov, which = 4)
plot(STAR_medn_aov, which = 5)
```

## Unequal Medians: Kruskal Wallis Test

~~Using the Kruskal-Wallis test at significance level 0.05 we can test
the following hypothesis:~~

$$ 
H_0: \text{Medians between all groups are equal.} \\ H_1: \text{Not all medians are equal.}
$$

~~We perform the Kurskal-Wallis test for both class type and school
type. For the class type compared to the median score we obtained a
significant p-value of 0.00117. For the urbanicity compared to the
median score we obtained a significant p-value of 8.091e-13. Thus we
found among class types there is a significant difference in the
difference between the three class types. Similarly a significant
difference was found between the medians of the various types of
schools.~~

```{r, warning=F, error=F, message=F, echo = F}
#H0: The medians of all groups are equal
kruskal.test(medn_score ~ g1classtype, data = STARt)

kruskal.test(medn_score ~ g1surban, data = STARt)

kruskal.test(medn_score ~ as.factor(g1schid), data = STARt)
```

## Dunn Test

~~Because we have broken away from our normality assumption (see
Sensitivity Analysis, Figure 10) and the Kruskal-Wallis rank sum test
has been reject we will perform a Dunn test at significance level~~
$\alpha=0.05$~~. A Dunn test is used when we have found indication that
there is a difference between one or more groups, as noted in the
previous Kruskal-Wallis test. This test we are performing will allow us
to pinpoint if any particular groups have a higher effect on median math
scores than another. The Dunn test is of use here due the departure from
normality experienced from our model, as the Dunn test is a non
parametric test.~~

~~We will test the following hypothesis:~~

$$ H_0: \text{There is no difference in median math scores between class types} \\ H_1: \text{There is a difference in median math scores between class types} $$

~~Based on our results we can see that small classes in comparison to
regular classes and regular + aide classes have a higher average median
score. No strong distinction between regular classes and regular + aide
classes can be made due to the high p-value.~~

```{r, warning=F, error=F, message=F, echo = F}
dnn_test = dunnTest(medn_score ~ g1classtype, "bonferroni", data = STARt)
dnn_test
```

# Discussion

In this project we have explored the effect of classroom size on the
test scores of students in relation to the urbanicity of the school they
are taught in. Student scores were aggregated into median scores per
teacher to avoid issues of correlation. We fit a fixed effect model,
using a variety of parametric and non-parametric tools to examine the
data.

With this work we have concluded that both class type and school
urbanicity have statistically significant main effects on the median 1st
grade math test scores of students. As well we can conclude that the
median 1st grade score of students taught in a small classroom setting
score higher on the math tests than students taught in other classroom
sizes.

It seems intuitive that with more individual attention a student has a
better potential to learn. Further analysis could be performed on the
size of the class size, perhaps by finding if smaller classroom sizes
continue to show improvement as class size shrinks or if at some point a
small class becomes detrimental. This may have issues being performed,
as an extremely small classroom deviates from normal class sizes which
may be harmful on education of students, something that the original
STAR investigation wished to avoid. Perhaps instead an investigation
could examine if the presence of additional teacher aides, and thus a
lower educator to student ration, could lead to better test scores.

It should be noted that while this investigation attempted to be robust
in dealing with issues of non-normality of the model, such an issue does
deviate from traditional testing methods. The non-parametric tests
employed do attempt to correct this, though it should be noted that the
Dunn test is especially conservative. Similarly while the work of Glass
et al. indicate that a skewness in the normality of the model should
have little effect on the F-test, they do not state there is no effect.
By taking into account this study and using it hand in hand with non
parametric models that reach the same conclusions we have hoped to
mitigate any issues that any individual test might hold.

It should also be noted that this investigation solely focused on the
math test scores of the first grade, with little consideration for any
other factors beyond the teacher and school setting of these students.
No examination of how these students performed in kindergarten is taken
into considering, nor even is the first grade test score looked at. Such
a narrow scope allows for a more targeted study, however stating that a
smaller classroom leads to higher overall academic performance is not
something this particular investigation can claim.

The STAR program's investigation into classroom size on educational
performance has allowed for a deep insight into the relationship between
these factors. The particular study we performed indicated that
classroom size and school setting had an impact on the first grade math
scores of students in Tennessee. With this study school administration
can work to adjust their schools to better the education of their
pupils, allowing for a brighter and more educated future generation.

# Acknowledgement {.unnumbered}

A special thanks to the following individuals: Sara A, Leena Q, Ian D,
Jaehwan K, Lubaba A, Patrick, Jack

# Reference {.unnumbered}

Achilles, C. M., & Shiffman, C. D. (2012). Class-size policy: The STAR
experiment and related class-size studies. *NCPEA Policy Brief*, *1*(2),
1–9.

Glass, G. V., Peckham, P. D., & Sanders, J. R. (1972). Consequences of
failure to meet assumptions underlying the fixed effects analyses of
variance and covariance. *Review of Educational Research*, *42*(3), 237.
<https://doi.org/10.2307/1169991>

Lix, L. M., Keselman, J. C., & Keselman, H. J. (1996). Consequences of
assumption violations revisited: A quantitative review of alternatives
to the one-way analysis of variance “f” test. *Review of Educational
Research*, *66*(4), 579. <https://doi.org/10.2307/1170654>

Tim. (2022, December 12). *Dunn’s test: Definition*. Statistics How To.
<https://www.statisticshowto.com/dunns-test/>

<https://stackoverflow.com/questions/66951931/ordering-boxplots-based-on-two-factor-variables-in-x-axis-with-ggplot2>

# Session info {.unnumbered}

[Report information of your `R` session for
reproducibility.]{style="color:blue"}

```{r}
sessionInfo()
```

# Code Appendix

```{r get-labels, echo = FALSE}
labs = knitr::all_labels()
labs = labs[!labs %in% c("setup","getlabels","allcode")]
```

```{r all-code, ref.label = labs, eval = FALSE}
```
