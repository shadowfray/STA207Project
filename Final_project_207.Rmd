---
title: "Personal Learning: The Effects of Class Size and Urbanicity on 1st Grade Math Performance in STAR Study"
author: "Benjamin Jewell"
date: "March 18th 2024"
output:
  html_document:
    df_print: paged
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r, echo=F, message=F, error=F, warning=F}
library(AER)
library(ggplot2)
library(tidyverse)
library(foreign)
library(gplots)
library(car)
library(FSA)
library(knitr)

STAR = read.spss("../STAR_Students.sav", to.data.frame = T)
STAR$g1schid = as.factor(STAR$g1schid)
```

------------------------------------------------------------------------

------------------------------------------------------------------------

# Abstract

~~The original STAR study worked to find if the size of a classroom for
kindergartners through first graders had an impact on their future
academic performance. In this paper we will study if the classroom size
and setting of the school impacts their math test score. The median math
score of students grouped by their teacher was used to measure their
performance. A fixed effects model was fit to the data, though
deviations from the normality assumption were detected so non-parametric
tests were employed. Both classroom size and school setting were found
to be impactful to first grade math scores, and among class types it was
concluded that small classrooms lead to higher math scores compared to
other class types.~~

# Introduction

This study is seeking to answer two questions: Is any differences in
math scaled scores in 1st grade across class types? As well as, which
class type is associated with the highest math scaled scores in 1st
grade? One can likely believe anecdotally that in a class packed with
students and a sole teacher the quantity of education is likely
diminished compared a class where students get individual attention.
This study seeks to examine this belief, in comparing both small classes
to regular classes, and regular classes with regular classes that also
have a teacher's aide. Both of these approaches lead to balancing the
student to educator ratio, though in different manners. In performing
this study we can hope to discover a better way to educate students to
ensure their academic success. Not only can this help their future
careers and opportunities, but also leads to a more educated future for
society.

[What is OUR question? What are our assumptions?]

# Background

Known as the Tennesses Student/Teacher Achievement Ratio study (STAR),
this project focused on the benefits of classroom sizes for students and
was a study conducted in 1985 to 1989. The study focused on students in
kindergarten and first through third grade, recording their math and
reading scores those four years. Data was collected for a wide number of
factors, from class size, to the education of a given teacher, and many
more. Further analysis would be conducted over twenty five years to
examine the long term benefits for these students.

The STAR study sought to further understand the difference in classroom
size, which until then had confusing and unclear data due to various
counting methods (Achilles, 2012, p. 1). In performing their own
experiment the study sought to find a clear and definitive answer to the
effects of classroom size on the education of their students. In
conducting this experiment it was hoped that a stronger method for
teaching children could be discovered to better not only their own
educational and professional achievements, but also to help fashion a
more educated society.

# STAR Experimental Design

The target students in this study were in seventy nine different schools
throughout Tennessee in kindergarten, who would be tracked for their
adolescent academic lives. These schools used in the study were of a
variety of types including inner city schools, rural schools, urban
school and suburban schools. Any appropriate school within Tennessee
could join the study, allowing for a broader scope in what schools were
included. Schools had to meet a minimum pupil count in the first year to
join, allowing for seventy nine schools to be part of the study the
first year. Due to the variety of schools and locations used the study
believes it has allowed for a reasonable representation of different
incomes and ethnicity among the students (Achilles, 2012, p. 6).

The study tracked students starting in kindergarten, with varying
classroom types prescribed to them by the STAR project through third
grade. With 7,000 students to begin with, these students were each
randomly assigned to one of three classroom types: small classrooms
(13-17 students), regular classrooms (22-25 students) and regular
classrooms with an aide to assist the teacher. Similarly to the
students, teachers were randomly assigned classroom types to avoid any
issues with teachers specializing in teaching particular sizes of
classrooms. Besides the change to classroom size no other changes were
made to allow for a normal school environment.

Compared to many similar studies the STAR project sought to focus
specifically on the size of a given class rather than the pupil-teacher
ration (PTR). This allows for a clear understanding of how many students
are involved in a given classroom, for a class with two teachers and
sixty students will have the same PTR as a class with one student and
thirty students. The use of classroom size in STAR allows for a more
direct analysis on the educational benefits of the number of students in
a given classroom.

It should be noted that while the STAR program began with 7,000 students
they did not retain all students over the course of the study. As the
program did not force students to stay in the project students might
leave throughout the study for a variety of factors such as moving to a
different city or state, or having to drop out of school. By beginning
first grade three of the seventy nine schools had dropped from the
program.

Not only did students leave the program, but because the STAR study
examined specific schools and not students, any students who joined a
STAR school were enrolled into the study. These students would not have
had the controlled classroom size from previous years of the study, and
could only benefits from the years they were in the study but no note is
made to exclude them. To compound this issue, at the time this study was
conducted kindergarten was not mandatory in Tennessee, meaning many
students joined the program for the first time in first grade and missed
a whole year of STAR class size.

[Why do we still use the study?]

# Initial Analysis

Previous examinations have been made of this data set in an initial
analysis report. Initial inquiries into the first grade math scores
looked for a relationship between the class type and specific schools.
This investigation used an additive two way ANOVA model with no
interaction. While the model indicated that both class type and school
ID had an effect on the first grade math scores there were several
issues with the model used. In investigating model assumptions several
were found to be broken, namely the normality and equal variance. As
such any conclusions that this initial analysis report might have made
are not valid, only serving to familiarize us with the problems we will
face in this study.

[How did we mess with that data here? (Also dropped NAs)

# Descriptive analysis

For our analysis we will be concerning ourselves with five variables, ,
`g1tmathss` , `g1tchid`, `g1classtype`, `g1schid`, and `g1surban`. Each
of these five variables corresponds to a student in the first grade,
tracking their progress in the STAR project. In order our five variables
are:

-   `g1tmathss`: The math score of a given student in the first grade.

-   `g1schid`: The ID of the school a given student is at.

-   `g1tchid`: The ID of the teacher who is teaching a given student.

-   `g1classtype`: The type of class the student is enrolled in, with
    levels `small`, `regular` and `regular+aide`.

-   `g1urban`: The urbanicity of the school the student is enrolled in,
    with levels `inner city`, `suburban`, `rural` and `urban`.

## Summary of 1st Grade Math Scores

We can see that we have a mean math score among first grade students of
530.53, a median score of 529, a lower quartile of 500 and higher
quartile of 557. There is a standard deviation of 43.11 among the grade
1 math scores. Of particular note is the large amount of missing data
points in the data, as denoted by `Total_NAs` below, making up more than
a quarter of the data. This is no insignificant portion of the data, so
we will need to investigate this further below.

```{r, warning=F, error=F, message=F, echo = F}
#Summary stastistics of 1st year math scores
g1smath_stats = data.frame(
        Mean = mean(STAR$g1tmathss, na.rm = T),  
        Stddev = sd(STAR$g1tmathss, na.rm = T),
        q25 = quantile(STAR$g1tmathss, probs = 0.25, na.rm = T), 
        q50 = quantile(STAR$g1tmathss, probs = 0.50, na.rm = T), 
        q75 = quantile(STAR$g1tmathss, probs = 0.75, na.rm = T))
colnames(g1smath_stats) = c('Mean', 'Standard Deviation', 'Q25%', 'Q50%', 'Q75%')
rownames(g1smath_stats) = c('')
kable(g1smath_stats, captain = 'Table 1: Summary statistics of 1st grade math scores')

  
kable(c(Total_NAs = sum(is.na(STAR$g1tmathss)), #How many NAs in math scores
Total_data = length(STAR$g1tmathss)),
col.names = 'Missing Values',
caption = 'Table 2: Missing Values Summary')

```

## Missing Values and STAR Project Inconsistencies

### Analysis of Missing Values

Looking at the math scores it can be noted that 43.12% (3043 students)
of these values are missing. However upon investigating, a majority of
these missing values are from students who have left the STAR program by
the first grade. Removing these non-STAR students from our data we can
find that this reduces the number of missing values to 3.38% (256
students), a much smaller proportion of the data.

```{r, echo = F}
#How many math scores are NA out of the total?
tot_raw_na = sum(is.na(STAR$g1tmathss)) / length(STAR$g1tmathss) #BAD!

#Alternatively using the STARGS1 Flag 
tot_star_na = sum(is.na(STAR$g1tmathss[which(STAR$FLAGSG1 == 'YES')])) / length(STAR$g1tmathss[which(STAR$FLAGSG1 == 'YES')])


#Number of non-NA values in g1math
not_na = 1 - tot_raw_na
#Number of NA values in g1math that are enrolled in star (FLAGSG1 = T)
star_na = sum(is.na(STAR$g1tmathss[which(STAR$FLAGSG1 == 'YES')])) / length(STAR$g1tmathss)
#Number of NA values in g1math NOT enrolled in star
not_star_na = tot_raw_na - star_na 


#Piechart of missing proportions
tot_star_df = data.frame(group = c('Missing Value in STAR', 'Existing Values in STAR', 'Missing Values not in STAR'),
  perc = c(star_na, not_na, not_star_na))


#Pie chats for data
ggplot(tot_star_df, aes(x="", y=perc, fill = group)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start=0) +
  xlab('') + ylab('') +
  ggtitle('Figure XX: Proportion of Missing 1st Grade Math Scores')

#Table of values
na_df = data.frame(c(tot_raw_na, tot_star_na))
colnames(na_df) = c('Percent')
rownames(na_df) = c('Percentage of Missing Math Values', 'Percentage of missing Math values of enrolled stuents')
kable(na_df, caption = 'Table 3: Missing Values for STAR and non-STAR students')
```

### Class Size Inconsistencies

In our examination we will want to investigate the data to ensure that
it complies with the definitions set up in the project. As explained
STAR Experimental Section, the size of a small class is defined as a
class with 13 to 17 students, while the other classes contain 22 to 25
students. In examining the data we can find that a number of classes
deviate from these assumptions. In total only 89.14% of classes defined
as small are actually small, and only 58.50% of regular sized classes
are actually regular. To ensure that proper adherence to the
experimental design is met those classes that fail to adhere to their
initial definition will be removed from our investigation.

```{r}
STARsmall = filter(STAR, (g1classtype == 'SMALL CLASS' & g1classsize %in% 13:17))
STARreg = filter(STAR, (g1classtype != 'SMALL CLASS' & g1classsize %in% 22:25))

nrow(STARsmall) / nrow(filter(STAR, g1classtype == 'SMALL CLASS'))
nrow(STARreg) / nrow(filter(STAR, g1classtype != 'SMALL CLASS'))
```

### Consistent Class Type

To ensure that students are complying with the class type they have been
assigned to, it must be checked that students are enrolled in the same
size of class across the two years we are examining. Any students who
deviate from this requirement will have a mixed education and cannot be
clearly placed in one education type or another. As such we will drop
these mixed students from our study.

There are two major notes to in doing this that must be stated. First,
this process will remove any students not enrolled in first grade or
kindergarten. As we are looking at the effects of a given class type
across both kindergarten and first grade there appears to be little
issue here. It should be noted however that in Tennessee when the study
began kindergarten was not mandated, thus limiting the scope of this
study to kids whose parents sent them to kindergarten. Secondly, after
the first year of the STAR study decided to modify their experiment in
the following manner: Half of the students in the 'regular' class size
were randomly assigned to 'regular + aide' class size, while half of the
'regular + aide' students were randomly selected to be moved to the
'regular' class size. Due to this deviation from the initial study
designs we will have to drop these students as they deviate from our own
investigation on the effects of subsequent class sizes.

```{r}
#Our new reduced data set
STARc = filter(STAR, (g1classtype == 'SMALL CLASS' & g1classsize %in% 13:17) | (g1classtype != 'SMALL CLASS' & g1classsize %in% 22:25))
STARc = filter(STARc, g1classtype == gkclasstype)
```

### Dropping Data Points

In setting our caveats for our investigation we can see that this
resulted in the side effect of cutting down the vast majority of missing
values from our 1st grade math scores. Where before 43.13% of our data
points were missing values (Table 3), we have reduced the number of
missing points down to just 1.5% of our data. This is half the number of
missing data points we achieve by not including students not enrolled in
the STAR program in first grade. As seen in Figure AA few schools are
missing more than one value, with the most missing being four values.

As the proportion of missing students makes up less than 2% of our data
set we can be assured that we have enough remaining students per school
to continue with our study. As such the remaining 1.5% of students with
missing 1st grade math scores will be dropped from the study we are
conducting.

```{r, warning=F, error=F, message=F, echo = F}
#How many missing values now?
sum(is.na(STARc$g1tmathss)) / length(STARc$g1tmathss)

#Do any schools have more NA values than others?
na_star = STARc %>%
  group_by(g1schid, g1tmathss) %>%
  summarize(na_count = sum(is.na(g1tmathss)))

#Histogram of missing values
hist(na_star$na_count[which(is.na(na_star$g1tmathss))], breaks = 1:30,
     xlab = 'Frequecy of NA values',
     main = 'Figure AA: Frequency of missing values among Schools')

```

## Aggregated Teacher Statistic

In examining the 1st grade math scores between students we cannot simply
use the individual scores due to high correlation between students in a
single class. Because they are taught in the same setting and in the
same manner a high correlation develops between them, instead the median
score of such a class could be considered to deal with this problem. To
take the problem a step further, students taught by the same teacher are
likely to have issues of correlation as well since teaching style likely
only differs so much between classes taught by the same educator.

Taking this all into consideration we will be using the median score of
all students taught by a single teacher as our statistic of interest in
this study. The median was chosen over the mean as it will not be
effected by outliers, and in previous investigations it was found that
using the mean results in a model that deviates from key model
assumptions.

```{r, warning=F, error=F, message=F, echo = F}
STARt = STARc %>% 
  group_by(g1tchid, g1classtype, g1schid, g1surban) %>%
  drop_na(g1tmathss) %>%
  summarize(medn_score = median(g1tmathss, na.rm = T),
            )
minicdat = STARc %>% 
  group_by(g1tchid, g1classtype, g1schid, g1surban) %>%
  drop_na(g1tmathss) %>%
  summarize(medn_score = median(g1tmathss, na.rm = T))

STARt$g1schid = as.factor(STARt$g1schid)
STARt$g1tchid = as.factor(STARt$g1tchid)
```

### Median Math Score: Summary Statistics

Examining the new median statistic score per teacher we can find that
there is a median score of 532, a standard deviation of 26.74, a lower
quarter of 512 and upper quartile of 549. As seen in Figure 2 the median
math score per teacher is somewhat normally distributed, but appears to
have a heavy tail.

```{r, warning=F, error=F, message=F, echo = F}
#Statistics for each Teacher
teacher_dat = data.frame(c(mean(STARt$medn_score),
  sd(STARt$medn_score, na.rm = T),
  q25 = quantile(STARt$medn_score, probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score, probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score, probs = 0.75, na.rm = T))) #75th quantile
colnames(teacher_dat) = c('Median Test Scores per Teacher')
rownames(teacher_dat) = c('Mean', 'Std Dev', 'Q25%', 'Q50%', 'Q75%')
kable(teacher_dat, caption = 'Table 4: Summary Statistics for Median Scores per Teacher')

#Histogram of median math scores per teacher
ggplot(data = STARt, aes(medn_score)) +
  geom_histogram(bins = 30) +
  xlab('Mean Math Scores per Teacher') +
  ggtitle('Figure 2: Histogram of Median Math Scores per Teacher')
```

### Class Types: Summary Statistics

Examining the statistics of various class types we can see in the
boxplot comparison small classes have higher medians and quartiles
compared to the other two class types (Figure 3). Actual testing will
need to be performed to confirm this visual inspection. The various
summary statistics for each class type can be seen in Table 5, which
supports our visual inspection of the boxplots.

```{r, warning=F, error=F, message=F, echo = F}
#Statistics for `small` classes
small_class_stats = c(mean(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS']),
  sd(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS'], na.rm = T), 
  q25 = quantile(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS'], probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS'], probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score[STARt$g1classtype %in% 'SMALL CLASS'], probs = 0.75, na.rm = T)) #75th quantile 

#Statistics for `regular` classes
reg_class_stats = c(mean(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS']),
  sd(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS'], na.rm = T), 
  q25 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS'], probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS'], probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR CLASS'], probs = 0.75, na.rm = T)) #75th quantile 

#Statistics for `regular+aide` classes
rgaide_class_stats = c(mean(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS']), 
  sd(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS'], na.rm = T), 
  q25 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS'], probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS'], probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score[STARt$g1classtype %in% 'REGULAR + AIDE CLASS'], probs = 0.75, na.rm = T)) #75th quantile 

classdat_df = data.frame(small_class_stats, reg_class_stats, rgaide_class_stats)
colnames(classdat_df) = c('Small Class', 'Regular Class', 'Regular + Aide Class')
rownames(classdat_df) = c('Mean', 'Std Dev', 'Q25%', 'Q50%', 'Q75%')

kable(classdat_df, caption = 'Table 5: Summary Statistics per Class Size')

ggplot(data = STARt, aes(g1classtype, medn_score)) + 
  geom_boxplot() +
  xlab('Class Type') +
  ylab('Average Test score Year 1') +
  ggtitle('Figure 3: Median combined test Score per class type')
```

### School Urbanicity: Summary Statistics

Below we can see the summary statistics for the urbanicity of each
school can be seen in Table 6. As seen in Figure 4 we have compared the
median math scores per individual school, with coloring per school
urbanicity to differentiate between them. Figure 5 displays a boxplot of
median math scores per urbanicity level, where we can see that inner
city schools have lower medians than the other three settings which are
approximately equal.

```{r, warning=F, error=F, message=F, echo = F}
sum_stats = function(lbl){
  c(mean(STARt$medn_score[STARt$g1surban %in% lbl]), 
  sd(STARt$medn_score[STARt$g1surban %in% lbl], na.rm = T),
  q25 = quantile(STARt$medn_score[STARt$g1surban %in% lbl], probs = 0.25, na.rm = T), #25th quantile 
  q50 = quantile(STARt$medn_score[STARt$g1surban %in% lbl], probs = 0.50, na.rm = T), #50th quantile 
  q75 = quantile(STARt$medn_score[STARt$g1surban %in% lbl], probs = 0.75, na.rm = T)) #75th quantile 
}

#Summary Stats for School Settings
inner_city_dat = sum_stats('INNER CITY') 
suburban_dat = sum_stats('SUBURBAN')
rural_dat = sum_stats('RURAL')
urban_dat = sum_stats('URBAN')

school_setting_df = data.frame(inner_city_dat,
                               suburban_dat,
                               rural_dat,
                               urban_dat
                               )
colnames(school_setting_df) = c('Inner City', 'Suburban', 'Rural', 'Urban')
rownames(school_setting_df) = c('Mean', 'Std Dev', 'Q25%', 'Q50%', 'Q75%')
kable(school_setting_df, caption = 'Table 6: Summary Statistics per School Setting')

sortedSTAR = STARt[order(STARt$g1surban), ]
lvlinfo = sortedSTAR %>% 
      arrange(g1surban, g1schid) %>%
      pull(g1schid) %>%
      unique()

STARt %>%
  mutate(g1surban = as.factor(g1surban),
         g1schid = factor(g1schid, levels = lvlinfo)) %>%
  ggplot(aes(x = g1schid, y = medn_score, fill = g1surban, color = g1surban)) +
  geom_boxplot() +
  xlab('') +
  ylab('') +
  ggtitle('Figure XX: Median Score between Schools') +
  theme(axis.text.x = element_blank())
  
#Boxplot of median score per school
#ggplot(data = sortedSTAR, aes(x = reorder(g1schid, g1surban),, y = medn_score, #color = g1surban)) + 
#  geom_boxplot() + 
#  xlab('1st grade School') +
#  ylab('Average Test score Year 1') +
#  ggtitle('Figure 4: Median Combined Test Score per School Year 1') 

#Boxplot of median score per school setting
ggplot(data = STARt, aes(g1surban, medn_score, color = g1surban)) + 
  geom_boxplot() + 
  xlab('1st grade School') +
  ylab('Average Test score Year 1') +
  ggtitle('Figure XX: Median combined 1st grade test score by urbanicity')
```

### Examining distribution of Urbanicity and Schools

It should be noted that in adhering to the caveats of our study the size
of our dataset has been trimmed down to a little over half the initial
data. As such when examining the median scores of a given teacher we
should ensure that there is enough data to move forward. Below two plots
have been made to highlight the portion of students in each treatment
when using school ID compared to urbanicity as our variable to represent
location. As we can see not all levels of class size can be found in
each school (Figure XY). Comparatively when representing school location
by urbanicity we have a minimum of five units per treatment, though it
should be noted that there is much higher proportion of rural classes
(Figure XZ).

```{r}
barplot(table(STARt$g1classtype, STARt$g1schid), 
        main = 'Figure XY:Treatments of School ID and Class Size')
barplot(table(STARt$g1classtype, STARt$g1surban),
        main = 'Figure XZ: Treatments of Urbanicity and Class Size')
```

# Inferential analysis

## Modeling

In beginning this analysis during our initial analysis report it was
directed to use a model that accounted for class type and individual
schools as factors of our model. In our analysis of the data however we
have seen that to ensure that all treatments are represented in our
model the use of urbanicity will surplant the use of school ID. Because
each school belongs to a single type of urbanicity there exists a
straight forward mapping between individual schools and the level of
their urbanicity. In using the the urbanicity of the school readers of
this report may be able to gauge how effective class size can be for
their specific school.

We will be fitting a fixed effect model for the data given, using the
class type and school urbanicity as our independent variables and the
median math score per teacher as our response variable.

### Model Assumptions

An imbalanced fixed effect model was chosen to be fit to our data, with
the following equations:

$$
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk} \\
i \in \{1,2,3\}, j =1,...,4, k=1,2,...,n
$$

In this case our variables are:

-   $\mu_{..}$ : Mean response across all indexes.

-   $\alpha_i$ : The main effects of the fixed variable school
    urbanicity.

-   $\beta_j$ : The main effects of the fixed variable class type.

-   $(\alpha\beta)_{ij}$ : The interaction term between class type and
    school urbanicity.

-   $\epsilon_{ijk}$ : The random error for entry (i,j,k).

### Model Fitting

We will be performing an F-test for both of our variables, class type
and school setting, to see if they are significant at $\alpha = 0.05$.
We will be testing the following hypothesis:

$$
H_0: \alpha_i = 0 \text{ }\forall i. \\
H_1: \text{Not all }\alpha_i\text{ equal zero.} 
$$

As well as:

$$
H_0: \beta_j = 0 \text{ }\forall j. \\
H_1: \text{Not all }\beta_j\text{ equal zero.}
$$

As we can our tests return a p-value of XXX, XXX, XXX respectively,
indicating that both factors of school setting and classroom have main
effects. Note that this test assumes normality from the model, which we
will discuss further in the sensitivity section of this report.

A test for interaction terms was also performed at significant level
$\alpha = 0.05$, testing the following hypothesis:

$$
H_0: (\alpha\beta)_{ij} = 0 \\
H_1: \text{not all } (\alpha\beta)_{ij} \text{ are zero.}
$$

Via the F-test we report a p-value of XXX, indicating that we reject the
null hypothesis. As such there is interaction between classroom type and
school urbanicity. That is, certain school urbanicities combined with
certain class sizes will have influence on the median math scores.

```{r, warning=F, error=F, message=F, echo = F}

STARaov.m1 = aov(medn_score ~ g1classtype * g1surban, data = STARt)
STARaov.m2 = aov(medn_score ~ g1classtype + g1surban, data = STARt)
STARaov.m3 = aov(medn_score ~ g1schid, data = STARt)

anova(STARaov.m1, STARaov.m2) #Pr = 0.18, Use simpler model
anova(STARaov.m2, STARaov.m3) # Pr < 0.05, Use more complex model

#Our final model
STAR_medn_aov = aov(medn_score ~ g1classtype * g1surban, data = STARt)
summary(STAR_medn_aov)
```

## Effects Plots & Interaction Plot

An investigation into the effects leads to examining the main effects
plot. The main effects of class type can be seen in Figure 6, where it
appears that a smaller class type has a strong effect on median math
score compared to the other two class types, further supporting our
suspicions drawn from the boxplots earlier (Figure 3). This will be of
particular note when answering our secondary question below. In Figure 7
we can see the main effects of the school setting, with a Inner City
schools having a significant decrease in median math scores compared to
the other school locations. Finally an interaction plot is constructed
(Figure 8), allowing us to see that there is little interaction between
school setting and class type, as our F-test previously confirmed.

```{r, warning=F, error=F, message=F, echo = F}
#Effects & Interaction Plots
plotmeans(STARt$medn_score ~ STARt$g1classtype, main = 'Figure 6: Main Effects of Class Type')
plotmeans(STARt$medn_score ~ STARt$g1surban, main = 'Figure 7: Main Effects of School Setting')
interaction.plot(STARt$g1surban, STARt$g1classtype, STARt$medn_score, main = 'Figure 8: Interaction Plot between Class Type and School Setting')
```

## Secondary Question of Interest:

Let us perform the Tukey Range test at significance level
$\alpha = 0.05$. The Tukey Range test examines all pairwise differences
in the means between different levels in a given treatment. This test
has the following assumptions

-   $Y_{ijk}$ are independent within their group and between groups

-   Each group is normally distributed

-   Homogeneity of variance between the group variances.

The Tukey Range test will be used to perform an examination to discover
if any class type has a higher mean summary score than other class
types. In particular we will be testing the following null hypothesis,
$H_0: \mu_i = \mu_{i'}$ where i and i’ are the three levels of the STAR
class type.

```{r}
tkhsd = TukeyHSD(aov(medn_score ~ g1classtype, data = STARt))
tkhsd

plot(tkhsd)

shapiro.test(minicdat$medn_score[which(minicdat$g1classtype  == 'SMALL CLASS')])
shapiro.test(minicdat$medn_score[which(minicdat$g1classtype  == 'REGULAR CLASS')])
shapiro.test(minicdat$medn_score[which(minicdat$g1classtype  == 'REGULAR + AIDE CLASS')])
```

In examining the difference in mean between regular classes and small
classes with a p-value less than 0.01 there was a negative difference in
means, indicating we reject our null hypothesis. Between regular classes
and small classes a p-value of 0.39 was reported, while between
regular + aide and regular classes there was a p-value of 0.11. Thus in
both these cases we fail to reject our null hypothesis.

Among students that participated in the same classroom setting both
years, students who are a part of smaller classes have higher first
grade math scores than those in a regular setting. Due to a lack of
significance no comparison can be made between regular and regular +
aide classes or between small classes and regular + aide classes.

# Sensitivity analysis

## ANOVA Assumptions

Let us test the various assumptions of our model to ensure that we do
not deviate away from them.

### Normality in Residuals

One assumption of particular note here is that of normality. In
examining our residuals vs fitted plot (figure XX) we can see that there
is little pattern to the data points, indicating no major issues with
variance or mean. However in examining our residuals via histogram and
qqplot (Figure XX, Figure XX) we can see that there is a deviation from
normality. Our histogram is right skewed, one piece of evidence of
departure from normality. This is further confirmed by performing a
Shapiro-Wilks test. At significant level 0.05 the test rejects our null
hypothesis with a p-value of 0.04, concluding that our data is
non-normal.

While the breaking of the normality of assumption is something to take
note of, it is not as a big of a concern as previously led to believe.
As stated in Glass et al. (1972) "Skewed populations have very little
effect on either the level of significance or the power of the
fixed-effects model F-test" (Glass et al, 273). Because this skewness is
not particularly strong we can move forward knowing this departure of
normality will not strong effect our F-tests.

```{r, warning=F, error=F, message=F, echo = F}
plot(STAR_medn_aov, which = 1, main = 'Figure XX: Median Model Residuals vs Fitted plot')
hist(residuals(STAR_medn_aov), main = 'Figure XX: Histogram of Median Model Residuals', xlab = 'Residuals')
qqnorm(residuals(STAR_medn_aov), main = 'Figure XX: QQplot of Median Model')
qqline(residuals(STAR_medn_aov))

shapiro.test(residuals(STAR_medn_aov)) #P > 0.05 = Normality
```

### Levene Test for Equal Variance

Testing for equal variance among groups, a Levene Test was performed at
significance level 0.05. We will be testing if the variance among
populations is equal, with the follow null and alternative hypothesis.

$$
H_0: \sigma^2_1 = ... = \sigma^2_n
$$

$$
H_1: \sigma^2_i \neq \sigma^2_j \text{ for all } i\neq j
$$

The results of the Levene test indicate no presence of unequal variance
among levels of class type, nor among levels of school setting. As equal
variance is a key assumption in our fixed effects model this test
indicates we need not worry about such depatures.

```{r, warning=F, error=F, message=F, echo = F}
leveneTest(medn_score ~ g1classtype * g1surban, center = median, data = STARt) #P < 0.5 = unequal variance
```

### Independence

Unfortunately because we are only involved in analyzing the data and not
designing the experiment we must trust in how the Tennessee general
assembly set up the experiment. Independence can be seen in the random
assignment of class type to each class. Any correlation that might be
present among students of the same teacher has also been eliminated by
the use of our median summary statistic.

### Cook's Distance: Influential Points & Outliers

Using Cook's distance one can look for outliers and influential points
that might effect our model. The following plot, Figure 12, indicates
that there are few points that differ from the major of values. Those
few values that have a large Cook's distance compared to the rest of the
data set have values so small they don't approach being influential,
thus they are neither outliers. Thus no adjustments need to be made to
deal with influential points or outliers.

```{r, warning=F, error=F, message=F, echo = F}
par(mfrow = c(1,2))
plot(STAR_medn_aov, which = 4)
plot(STAR_medn_aov, which = 5)

ckd = cooks.distance(STAR_medn_aov)

influential <- as.numeric(names(ckd)[(ckd > 1)])
influential = influential[!is.na(influential)]

```

## Sensitivity of Model

[Perform sensitivity]

# Discussion

In this project we have explored the effect of classroom size on the
test scores of students in relation to the urbanicity of the school they
are taught in. Student scores were aggregated into median scores per
teacher to avoid issues of correlation. We fit a fixed effect model,
using a variety of parametric and non-parametric tools to examine the
data.

## Study Conclusions

With this work we have concluded that both class type and school
urbanicity have statistically significant main effects on the median 1st
grade math test scores of students. Because the model contains an
interaction term we can also conclude that the effects of class size and
urbanicity can come together to have a greater or lesser effect on a
given treatment, that is certain combinations of factors may lead to
different results.

Regarding our secondary question we can conclude that the median 1st
grade score of students taught in a small classroom setting score higher
on the math tests than students taught in regular size classrooms. No
comparison can be made between regular classrooms and regular classrooms
with an aide. Similarly no comparison can be made between small
classrooms and regular classrooms with an aide. Because of this we
cannot say that a small classroom has a definitively better performance
than all other classrooms in this study.

## Limitations and Challenges

It should be noted that while this investigation attempted to be robust
in dealing with issues of non-normality of the model, such an issue does
deviate from traditional testing methods. While the work of Glass et al.
indicate that a skewness in the normality of the model should have
little effect on the F-test, they do not state there is no effect.

[Issues with STAR design]

[Issues with how I trimmed the data]

[Restate Urbanicity]

[Sensitivity Testing]

## Closing Remarks

~~The STAR program's investigation into classroom size on educational
performance has allowed for a deep insight into the relationship between
these factors. The particular study we performed indicated that
classroom size and school setting had an impact on the first grade math
scores of students in Tennessee. With this study school administration
can work to adjust their schools to better the education of their
pupils, allowing for a brighter and more educated future generation.~~

# Acknowledgement {.unnumbered}

A special thanks to the following individuals: Sara A, Leena Q, Ian D,
Jaehwan K, Lubaba A, Patrick, Jack

# Reference {.unnumbered}

Achilles, C. M., & Shiffman, C. D. (2012). Class-size policy: The STAR
experiment and related class-size studies. *NCPEA Policy Brief*, *1*(2),
1–9.

Glass, G. V., Peckham, P. D., & Sanders, J. R. (1972). Consequences of
failure to meet assumptions underlying the fixed effects analyses of
variance and covariance. *Review of Educational Research*, *42*(3), 237.
<https://doi.org/10.2307/1169991>

Lix, L. M., Keselman, J. C., & Keselman, H. J. (1996). Consequences of
assumption violations revisited: A quantitative review of alternatives
to the one-way analysis of variance “f” test. *Review of Educational
Research*, *66*(4), 579. <https://doi.org/10.2307/1170654>

Tim. (2022, December 12). *Dunn’s test: Definition*. Statistics How To.
<https://www.statisticshowto.com/dunns-test/>

<https://stackoverflow.com/questions/66951931/ordering-boxplots-based-on-two-factor-variables-in-x-axis-with-ggplot2>

<https://stats.stackexchange.com/questions/164099/removing-outliers-based-on-cooks-distance-in-r-language>

John Folger & Carolyn Breda (1989) Evidence from project star about
class size and student achievement, Peabody Journal of Education, 67:1,
17-33, DOI: 10.1080/01619569209538668,
<https://doi.org/10.1080/01619569209538668>

# Session info {.unnumbered}

[Report information of your `R` session for
reproducibility.]{style="color:blue"}

```{r}
sessionInfo()
```

# Code Appendix

```{r get-labels, echo = FALSE}
labs = knitr::all_labels()
labs = labs[!labs %in% c("setup","getlabels","allcode")]
```

```{r all-code, ref.label = labs, eval = FALSE}
```
